---
title: "3. Modelli Miscuglio"
author: "NikolayNikolaev"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction to models mixture
## 3.1 Introduction
Statistical models of finite mixtures (nite mixture models, McLachlan and
Peel (2000)) allow us to explore the structure of the data in inferential terms
and determine groups of units (clusters) assuming the population of
reference as heterogeneous. These allow you to classify units into
distinct homogeneous groups with respect to the characteristics of interest, in a way
similar to cluster analysis which allows you to group a set of units
in such a way that the units of the same group (called cluster) are more similar
among themselves than with those in other groups.


Consider, for example in a pharmacological treatment patients can
have particularly different reactions to the same drug (think of
adverse effects of the COVID-19 vaccine). Asthma is treated with theo lline
and the dose depends on individual weight therefore a pharmaco-kinetic model
(i.e. which concerns the study of the absorption and disposition of the drug
for the development of new drugs) that assumed an identical parameter
for all subjects it would be unrealistic. There is often a high variability between the units treated with respect to the following characteristics: rate of
absorption into the blood, metabolism and elimination of the drug (which means
ca biological transformation and volume with which the drug is eliminated from
body). In the context considered, it is model-based clustering because it is
the population is assumed as a mixture of subgroups and each component
(cluster) has a distinct density function. For other examples, see
Schlattmann (2009).

Mixture models allow us to estimate unknown subgroups in a certain context
collectivity (population of interest) classifying the homogeneous units within
a group and creating non-homogeneous groups between them. Sometimes it is very useful
distinguish sub-populations of units that characterize the universe of
reference. An example taken from recent studies concerns genetic polymorphism
which indicates the multiple forms that a gene takes on an individual level
or group. It mainly refers to the fact that there are multiple forms
of a single gene in the same environment and at the same time point. Yes
it therefore speaks of sequences of DNA variations that persist over time and
they do not imply advantages (unlike mutations). In the human liver they are
many polymorphisms have been observed with respect to a certain enzyme. Enzymes
present the same sequence but the polymorphic structure could influence
the way in which substances are absorbed into the body. In these
contexts identify which subjects have similar polymorphic structures
promotes a more precise knowledge of the phenomenon.

The mixture models with assumed components are presented below
with a Gaussian distribution. These models are widely used for
They can be tractable on a mathematical level, due to the fact that it is quite easy
obtain maximum likelihood estimates of the parameters. However how
explained below, the selection of the model, or the choice of the number
of the components (clusters) requires particular attention.

## 3.2 Mixture model for continuous random variables

In the following reference, Y indicates the random variable and y indicates one
its realization. A class of mixture distributions was initially
proposed by Pearson (1894), composed of only two components with
Normal distribution $\phi(y, ,\mu_1, \sigma_1^2)$ and $\phi(y, \mu_2, \sigma_2^2)$

$$f(y; \theta)= \pi \phi(y, \mu_1, \sigma_1^2) + (1-\pi)\phi(y, \mu_1, \sigma_1^2)$$
where $\theta$ we denote the vector of parameters characterizing the distribution of Y , with $\phi(y, \mu, \sigma^2)$ we denote the univariate Gaussian density while with, $\pi$ and $1-\pi$ the weight of the first and second components respectively is represented. The model parameters are as follows $\theta(\pi, \mu_1, \sigma_1, \mu_2, \sigma_2)$. Estimating parameters using the method of moments requires solving
a polynomial equation of the ninth degree while estimating it with the method of
Maximum likelihood is based on the Expectation-Maximization algorithm
(EM) (Baum et al., 1970; Dempster et al., 1977). For error estimation
standard of model parameters bootstrapping is used.

More generally, let Y be a continuous random variable with relative values
sample space; its distribution is a mixture of nite components
when
$$y(y, \theta) = \pi_1 f_1(y_1; \theta_1)+ ... + [i_k f_k(u_k. \theta_k)$$

where:
- $\pi_1..\pi_k$ - are the weights;
- $f_j; j=1...k$- represents the density (probability) of the component
jth of the mixture function $(f_j(.) > geq 0; \int f_j(y))= dy=1$ and $\theta_1...\theta_k$ 
are the specific parameters of each component;

- every weight is greater than zero $\pi_j > 0; j=1...k$

- the sum of the weights is constrained to one: $\sum_j \pi_j = 1$

If each component has the same density and is supposed to be known, for example
these belong to the same Gaussian parametric family, then
in the previous notation it is possible to avoid indexing the densities of the singles
components writing:

$$f(y; \theta) = \pi_1 f(y; \theta_1) + ... \pi_k f(y; \theta_k)$$

where $f(. ; \theta_j)$ indicate the j-th memeber of the family and $\theta_j$  represents
the vector of family parameters j. For further details please refer
to the works initially proposed in Ban eld and Raftery (1993) and Fraley and
Raftery (2002).
The following function is a mixture of two components:

$$f(y) = 0.4 \phi(y; 1,1) + (1-0.4) \phi(y;1,1)$$

it is a convex linear combination of Gaussian density functions that it generates
a bimodal mixture distribution representing a population
heterogeneous. 

Note the conceptual difference between modeling a phenomenon (e.g
example income) through a mixture model and through a distribution
univariate as that represented by the following lognormal density
representing a homogeneous population

$$f(x; \theta) = \frac{1}{\sqrt{2\pi}x \sigma} exp (-\frac{1}{2} (log x- \mu)^2)$$

whre $x \in (0; +\infty)$ and $\mu \in (-\infty; +\infty), \sigma >0$

### Recall: convolution of random variables
Mixture models represent situations of heterogeneity and therefore
they must be distinguished from the density which is obtained with the convolution of variables
random. Let $X_1...X_n$ independent and identically random variables
distributed and let S be the sum of these variables which in English is called n-fold convolution:


$$S = X_1 + ...+X_n$$

We know that the convolution of exponential random variables gives rise to a v.c. Gamma. The random variable $\chi^2 (\nu)$ it is a convolution of $\nu$ random variables
of Gauss squared. The Negative Binomial distribution NegBin(r,p) e
a convolution of r independent Geometric random variables Geom(p).
and identically distributed.

For example, the convolution of two independent random variables $X_1$ and $X_2$ with
Gaussian distribution such that $X_1 \sim N(0,1)$  and $X_2 \sim(3,1)$ it's the following:

$$S = a_1X_1 + a_2X_2$$
where

$$S \sim N(3a_2, a_1^2 + a_2^2)$$
That is, S does not represent a mixture unless constraints are placed
per unit sum for $a_1$ and $a_2$

## 3.3 Multivariate Gaussian distribution

In the following we recall the notation of the Gaussian distribution as a joint distribution of k independent and identically distributed random variables$X_1 ...X_k$ with k>2. This distribution is widely used for its simple mathematical formulation and for the property that is available
of vectors of independent and identically distributed random variables
then the sample mean in the case of large numbers tends to be
distributed as a multivariate Gaussian distribution.

In the following, we first introduce the bivariate, relative Gaussian distribution
to two random variables X and Y, and then the discussion is extended to the case of more than
two random variables.

## 3.3.1 Bivariate Gaussian distribution
Consider a double random variable (X; Y ) and therefore (x; y) is its own
realization and f (x; y) is the joint density function. Given a < b and c <
d constants, the probability P (a < X < b; c < Y < d) is de ned by the following
volume:

$$P(a< X < B, c < Y < d)= \int_a^b \int_c^d f(x,y)dx dy$$

In particular, we say that (X; Y) has a Gaussian (or Normal) distribution
bivariate if the random variable resulting from any linear combination of the
following type $W = aX + bY; \text{all of }a,b \in \mathcal{R}$has univariate Normal distribution.
The parameters of this distribution are the means $\mu_x, \mu_y$, the varinaces $\sigma_x^2, \sigma_y^2$
and the linear correlation coefficient $\rho_{xy}$ (tote that: $cov(X,Y)= \rho_{xy}\sigma_x \sigma_y$)

Therefore it is written in compact notation

$$(X,Y) \sim N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho_{xy})$$

and the density function is the following

$$f(X=x; Y=y) = \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1-\rho_{xy}^2}} exp(-g)$$
where

$$ g = \frac{\frac{(x-\mu_x)^2}{2\sigma_x^2} + \frac{(y-\mu_y)^2}{2\sigma_y^2} + \rho[\frac{(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}]}{1-\rho_{xy}^2}$$
- $-\infty < x$
- $y < +\infty$
- $-\infty < \mu_x, \mu_y < +\infty$
- $\sigma_X >0, \sigma_y > 0$
- $-1 < \rho_{xy} < 1$


Some properties are listed below:
â€¢ Variables with Gaussian distribution, if uncorrelated, are also independent
(a result that does not hold in general) and the marginal distributions
they are univariate Gaussians. Indeed, it is $(X,Y) \sim N(\mu_x; \mu_y, \sigma_x; \sigma_y \rho_{xy}=0)$
a bivariate Gaussian distribution with uncorrelated variables, then:

$$f(x;y; \rho_{xy}=0) = \frac{1}{2\pi \sigma_X \sigma_y} exp(\frac{(x-\mu)^2}{2\sigma_x^2} + \frac{(y-\mu_y)^2}{2\sigma_y^2})$$

$$= \frac{1}{\sqrt{2\pi \sigma_x}} exp[\frac{(x-\mu_x)^2}{2 \sigma_x^2}]  exp[\frac{(x-\mu_y)^2}{2 \sigma_y^2}] = f(x)f(y)$$

Pertanto $X  \bot Y$

- Furthermore, it is shown that, marginalizing with respect to X or Y , holds $X \sim N(\mu_x, \sigma_x^2)$ and $Y \sim N(\mu_y; \sigma_y^2)$. Also conditional distributions $(X|Y=y)$ and $(Y|X=x)$

they are univariate Normals. In particular, it is demonstrated that:

$$E[X|Y=y] = \mu_x + \frac{\rho_{xy} \sigma_x\sigma_y}{\sigma_y^2} (y-\mu_y) = \mu_x + \rho_{xy} \frac{\sigma_x}{\sigma_y} (y-\mu_y)$$


$$E[X|Y=y] = \mu_y + \frac{\rho_{xy} \sigma_x\sigma_y}{\sigma_y^2} (y-\mu_x) = \mu_y + \rho_{xy} \frac{\sigma_x}{\sigma_y} (y-\mu_x)$$

$$Var[X|Y=y] = \sigma_x^2 (1-\rho_{xy}^2)$$
$$Var[Y|X=x] = \sigma_y^2 (1-\rho_{xy}^2)$$

- The contour lines of a bivariate Gaussian distribution are some
ellipses; the x;y correlation coefficient de nes the direction of the axis
greater than the ellipse. Some cases are shown in Figure 3.2
examples.
- when k varies the volume of the ellipsoid changes, but the proportions between
the axes remain unchanged;
- the eigenvectors of the variance-covariance matrix define the equations
of the main axes of the ellipsoids;
- the eigenvalues of the variance-covariance matrix are proportional
to the squares of the lengths of the principal axes of the ellipsoids.



## 3.3.2 Multivariate Distribution
Generalizing the concepts expressed in the previous section, we can introduce
the multivariate Gaussian distribution as a distribution referred to the random vector
$\vec{Y}= (Y_1...Y_p) \sim N_p (\mu, \sum)$ where $\mu \in \mathcal{R^p}$
is the vector of the means and $\sum \in \mathcal{R^{p*p}}$ is the symmetric variance-covariance matrix and positive density. The inverse of the variance-covariance matrix, $\sum^{-1}$ it is known as matrix
of forecasts.

Recall: The variance-covariance matrix of random variables $X_1..X_n$ and defined as


$$
\begin{bmatrix}
    \sigma_11^2   & \sigma_{12} & ... & \dots & \sigma_{1n} \\
    \sigma_12     & \sigma_2^2 & \dots & \dots & \sigma_{2n} \\
    \vdots        &    \dots & \dots & \dots & \vdots \\
    \sigma_{1n}       & \sigma_{2n} & \dots & \dots & \sigma_{n}^2
\end{bmatrix}
$$
where $\sigma_i^2 = Var[X_i]$ and $\sigma_{ij} = \COv[X_i; X_j]; i \ne j$ It's a square matrix
symmetrical with $\sigma_{ij} = \sigma_{ji}$ and defined as semi-positive.


The density function is defined as follows

$$f(y)= f(y_1..y_p)= \frac{1}{\sqrt{(2\pi)^p det (\sum)}} exp(-\frac{1}{2}(y-\mu)^T \sum^{-1}(y-\mu))$$

where the exponent term coincides with the square of the Mahalanobis distance
and assumes a Chi-square distribution with p degrees of freedom:

$\bigtriangleup_{\sum}^2 (y,\mu)= (y-\mu)^T \sum^{-1}(u-\mu) \sim \chi_p^2$

since it is the sum of p standardized Normal random variables.
In the case of a Standard multivariate Gaussian distribution $Z \sim N(0; \sigma I_n)$
the density function reduces to

$$f(z)= \frac{1}{\sqrt{(2\pi)^p}} exp(-\frac{1}{2}z^T z)$$

Please remember that the non-diagonal elements of $\sum^{-1}$ express the correlation
nor partial. Therefore if an element external to the diagonal is equal to
zero then in the case of normal distribution the variables are independent
conditionally on the remaining $p -2$.



# DEMO:

Generation of determinations from Bivariate Normal
The mvtnorm::rmvnorm function (from the mvtnorm library) allows you to generate realizations
from a Gaussian bivariate (or multivariate) density distribution.
The function requires as input the vector of means and the variance and covariance matrix
Î£.

Considering the following joint distribution for the two causal variables

$$(X_1, X_2) \sim N(\mu_1, \mu_2, \sigma_1, \sigma_2, \sigma_{12})$$
the following values of the parameters $ð‘ (0, 0, 3, 3, 2)$ are fixed and therefore $\rho_{12} = 0.67$ (approximately)
that is, these are variables with a strong positive linear association.
The rmvnorm function requires as input:

- the number of elements to generate
- the vector of means
- the square matrix of variances and covariances sigma1

In the following code, notice that the variance and covariance matrix is specified as
matrix object with the matrix function


```{r}
require(mvtnorm)
sigma1 <- matrix(c(3,2,2,3), ncol=2); sigma1

```

The realizations of the Double Normal are obtained in the following way:

```{r}
set.seed(1234)
n <- 10
x <- rmvnorm(n, 
             mean=c(0,0), 
             sigma=sigma1)
x

```
The output object contains the 10 realizations from $(ð‘‹_1, ð‘‹_2)$, where the first column refers
to $ð‘‹_1$ and the second to $ð‘‹_2$.
The empirical variance and covariance and correlation matrices for the obtained realizations
they are as follows.
```{r}
cov(x)
cor(x)
```
The number of realizations is increased up to 2000 by fixing the averages of the two variables
random to zero, respectively and leaving the variance-covariance matrix unchanged.

```{r}
set.seed(1234)
n <- 2000
x <- rmvnorm(n, mean=c(0,0), sigma=sigma1)
cov(x)
cor(x)
```
The descriptive statistics of the empirical values and the variance-covariance matrix e are calculated
that of correlation

```{r}
require(skimr)
skim_without_charts(x)
```
```{r}
cov(x)
cor(x)
sd(x[,1])
sd(x[,2])
```
The adherence of the simulated values to those of the theoretical distribution is noted.

## Scatter diagram of the values obtained
Previous achievements can also be viewed through the scatter plot
two-dimensional


```{r}
plot(x[,1],
x[,2], main = "Realizzazioni da Normale Bivariata N(0,0,3,3,2)")
```

Note that the points are arranged in the shape of an ellipse with the focus centered on the averages.
### Level curves
The contour line corresponds to the geometric locus of the points on the identified plane
from the equation $ð‘“(ð‘¥_1, ð‘¥_2) = ð‘˜$ where $ð‘˜$ is the level. The number of levels is set by default by
contour function based on density.
These are the orthogonal projections of the curves obtained by intersecting the previous graph
with a plan.
As $ð‘˜$ or the height of the plane increases, the circles get closer to the center.

Values are generated and density determined


```{r}
x1 <- x2 <- seq(-10, 10, length = 51)
dens <- matrix(dmvnorm(expand.grid(x1, x2),
sigma = sigma1),
ncol = length(x1))
contour(x1,
x2,
dens,
main = "Livelli della dist. N(0,0,3,3,2)",
col="blue",
xlab = "x1",
ylab = "x2")

```

Note that geometrically the orientation of the ellipses allows you to establish whether the association
between the two random variables is positive or negative. The center and length are also of interest
of the half aces.

## Expectation-Maximization (EM) algorithm
Consider the following double-entry contingency table

$$
Right   Left     Center   Default 
------- ------ ---------- ------- 
     12 12        hmmm        12 
    123 123        123       123 
      1 1            1         1
$$

|$X|Z$| 1 | 2 | 3 |  |
|---|---|---|---|---|
| 1 |$y_{11}$ | $y_{12}$|$y_{13}$|$y_{1}$ |
| 2 |$y_{21}$ | $y_{22}$|$NA$    | $y_{2}$|
|   |$y_{.1}$ | $y_{.2}$|$y_{.3}$| $ y_{..}$|


$$ y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{1j}$$

where $\mu$ is an intercept, $\alpha$ is the parameter for the row and $\beta$ for the column, while the hypothesis for the
error term is that this component follows a Normal distribution $\epsilon_{ij} \sim N(0; \sigma^2)$.

Zero-sum constraints are placed on the parameters $\sum_i a_i = 0$ and $\sum_j \beta_j = 0$

In the following, the EM algorithm is used to estimate the model parameters and the
missing data through the values of the parameters obtained upon convergence of the algorithm.
The steps of the estimation procedure are as follows:

- An initial value is assigned to missing data or unobserved frequencies $y_{23}^{(0)}$.A generally used way to initialize the algorithm is to use the statistics values of the observed data. For example, you assign to $y_{23}^{(0)}$ the average value of
observed frequencies.
â€¢ at step E (Expectation): they calculate the parameter estimates using the known solutions
in closed form with the initial values being the following

$\hat{\mu} = \bar{y}$

$\hat{\alpha}^{(0)} = \bar{y_{i.}} - \bar{y}$

$\hat{\beta_j}^{(0)} = \bar{y_{.j}} - \bar{y_{.}}$


The missing value is imputed as follows: $y_{23}^{(1)} = \hat{\mu}^{(0)} + \hat{\alpha_i}^{(0) +\hat{\beta_j}^{(0)} }$

- at step M (Maximization): we proceed iteratively by recalculating the values of
parameters based on the imputed value $y_{23}^{(1)}$ until the value set for is reached
the convergence criterion.


In the following example, consider the frequencies that concern two variables: ð‘‹ assumption
of placebo or drug and ð‘Œ state of health (very bad, good, excellent). It is intended to apply the
previous model and impute the missing value relating to the frequency of those who have
taken the drug and have excellent health.
The table with the frequencies is constructed using the matrix function and adding the
missing value


```{r}
y <- matrix(c(10,15, 17, 22, 23, NA),2,3,byrow=TRUE); y
```

The following function called em1 assigns an initial value to the missing cell of the
table and reconstructs the value based on the closed-form solutions for the model parameters
(these are shown on page 69 of the handouts).
Note the use of the apply function

```{r}
em1 <- function(y23, y){
ystar <- y
ystar[2,3] <- y23
mu.hat <- mean(ystar)
alpha.hat <- apply(ystar, MAR = 1,
mean) - mean(ystar)
beta.hat <- apply(ystar, MAR = 2,
mean) - mean(ystar)
y23 <- mu.hat + alpha.hat[2] + beta.hat[3]
return(c(mu = mu.hat,
alpha = alpha.hat,
beta = beta.hat,
y23 = y23))
}
```

The function returns the values of the model parameters obtained with the fit solutions
closed

```{r}
em1(21,y)
```

The following function em.step calls the previous function em1 to repeat the procedure
calculation updating the estimates until convergence.
The estimates of the maximum likelihood parameters and the corresponding are obtained as output
value of the missing data.
The function requires 2 inputs:
â€¢ the values of the contingency table
â€¢ the tolerance level ðœ– to choose for the convergence of the EM algorithm. Among the values
possible is considered $\epsilon = 10^{-8}$ or 0.00000001 indicated below with the notation 1e-8


In the body of the function, the dist function is called to calculate the distance between the values
contained in the trace object at iteration h+1 and at the h-th iteration (as shown
in theory lecture notes).
Note the use of the while construct to increment the counter until convergence.

```{r}
set.seed(1832)
em.step <- function(y, epsilon= 1e-8){
trace <- NULL
convergenza <- FALSE
trace <- t(em1(y23 = mean(y, na.rm = TRUE), y = y))
y23id <- grep("y23", colnames(trace))
h <- 0
while(!convergenza){
h <- h + 1
trace <- rbind(trace,
em1(y23 = trace[h, "y23"],
y = y))
convergenza <- (dist(trace[h:(h+1), -y23id]) < epsilon)
}
return(trace)
}
```
The data matrix function applies

```{r}
em.step(y)
```

The object that has the value of each parameter as its row is obtained at each iteration of the algorithm. 
Convergence with respect to the established criterion $\epsilon < 1e^{-8}$ is reached after 49 iterations.
The model parameter estimates are as follows:

$\hat{\mu}= 19, \hat{\alpha_1}=-5, \hat{\alpha_2}= 5, \hat{\beta_1}= -3, \hat{\beta_2}= 0, \hat{y_{23}}=27$

## Trace plot
The graphical representation of the values obtained at each step of the algorithm, also called
trace plot, is done using the matplot function

```{r}
ris<- em.step(y)
matplot(ris[,-7], type = "l")
```

With the following syntax you define the labels and colors and add them to the graph together
to the legend.

```{r}
names1 <- expression(mu,
alpha[1],
alpha[2],
beta[1],
beta[2],
beta[3])
pal1<- c("red", "yellow", "green", "violet", "blue", "orange")
matplot(ris[,-7],
type = "l",
col = pal1,
lwd = 2,
lty = 1,
xlab = "Iterazioni dell'algoritmo EM",
ylab = "Stime dei parametri del modello")
legend(x = 0,
y = 15,
legend = names1,
lwd = 2 ,
col = pal1,
lty = 1,
horiz=TRUE,
cex=0.8)
```


The graph allows you to view all the values of each parameter estimated at each step.
The values vary particularly in the first 10 iterations and in the subsequent ones there is an adjustment
which concerns the last decimal places.

# Mixture density of Gaussian components
The density is graphed for some mixture model with two components
Gaussians for a specific value of the weight of each component.
The function funcmxn calculates the value of the mixture density considering.
The weight of the first component $\pi \in (0,1)$ indicate witn $p$.

The parameter values $\mu_1, \mu_2, \sigma_2, \sigma_2$ of the first and second components in the mu vectors
and sd.
Note that the function returns the value of the mixture density at a point denoted by f.

```{r}
funcmxn <- function(x, p, mu, sd){
f1 <- dnorm(x, mu[1], sd[1])
f2 <- dnorm(x, mu[2], sd[2])
f <- p*f1 + (1-p)*f2
f
}
```


In the following, the mixture density is represented for three distinct scenarios.
## Scenario 1
In the first scenario, a mixture with two components is considered $X_1$ and $X_2$ such taht $X_1 \sim N(11)$ and $X_2 \sim N(4,1)$ that is, with the same variability but with the weight of the first component
equal to $\pi_1 = 0.4$ and for the second $1- \pi_1 = 0.6$

Si determina il valore della densitÃ  nel punto ð‘¦ = 0.5.

```{r}
mu1 <- c(1,4)
sd1 <- c(1,1)
p1 <- 0.4
funcmxn(0.5,
p1,
mu1,
sd1)
```

A density value of 0.14 is obtained.
For a sequence of y values the corresponding value of the mixture density is obtained

```{r}
y1 <- seq(-5,10,0.01)
length(y1)
#> [1] 1501
pr1 <- funcmxn(x = y1,
p = p1,
mu = mu1,
sd = sd1)
require(skimr)
skim_without_charts(pr1)
```

It is noted that the density is concentrated between the values delimited by the interquartile difference.
The obtained values are plotted to visualize the shape of the mixture density

```{R}
plot(y1,
pr1,
xlab = "y",
ylab="Densita'",
lwd=3,
col="lightsteelblue1",
type = "l",
main="Miscuglio di N(1,1) e N(4,1) con peso 0.4")
```



Multimodality is noticeable.

Scenario 2
In the second scenario the following two components are considered   $X_1$ and $X_2$ such taht $X_1 \sim N(4,1)$ and $X_2 \sim N(4,64)$ which have the same mean but the variability of the second component is very high
wider than that of the first, the weight of the first component is $\pi_1 = 0.1$ and the second $1-\pi_1 = 0.9$

For $y=0.5$

```{r}
mu2 <- c(4,4)
sd2 <- c(1,8)
p2 <-0.1
set.seed(1235)
funcmxn(0.5,
p2,
mu2,
sd2)

```

the density value is lower than that calculated in the previous scenario.
A sequence of values with a wider range of variation is generated

```{r}
y2 <- seq(-30,40,0.01)
pr2 <- funcmxn(x = y2,
p = p2,
mu = mu2,
sd = sd2)
skim_without_charts(pr2)
```

The density is concentrated between 0.0002 and 0.024.
The graph allows you to detect the shape of the mixture density in the second scenario

```{r}
plot(y2,
pr2,
xlab = "y",
ylab="DensitÃ  miscuglio",
lwd=3,
col="orange",
type = "l",
main="Miscuglio di N(4,1) e N(4,64) con peso 0.1 ")
```

In this case a single maximum is observed.
The density is concentrated between

## Scenario 3

In the third scenario, the value of the mixture density of two components is considered  $X_1$ and $X_2$ such taht $X_1 \sim N(0,1)$ and $X_2 \sim N(0,9)$ which have the same mean but the variability of the second
component is higher, the weight of the first component is equal to that of the second $\pi_1= 0.5$.

For $y=0.5$

```{r}
mu3 <- c(0,0)
sd3 <- c(1,3)
p3 <-0.5
funcmxn(0.5,
p3,
mu3,
sd3)

```
the value is higher than the two obtained in example 1 and example 2.

For a sequence of y values (note that the sequence of values must be generated in relation
to the range of variation of the two variables in order to obtain the value of the mixture density
on the support considered) the corresponding density is calculated

```{r}
y3 <- seq(-10,20,0.01)
pr3 <- funcmxn(x = y3,
p = p3,
mu = mu3,
sd = sd3)
skim_without_charts(pr3)
```


The graph allows you to detect the shape of the mixture density

```{R}
plot(y3,
pr3,
xlab = "y",
ylab="DensitÃ ",
lwd=3,
col="Pink",
type = "l",
main="Miscuglio di N(0,1) e N(0,9) con peso 0.5"
)
```



## Univariate mixture model with two Gaussian components

I dati riportati in datacol.Rdata riguardano alcuni individui selezionati per uno studio al
fine di valutare il ruolo del colesterolo come fattore di rischio per le malattie cardiovascolari.
I valori del colesterolo (espressi in mg su dl di plasma) sono riferiti sia ai maschi (variabile
binaria sex codificata con 1 per maschi) che alle femmine.

```{r}
load('data/datacol.Rdata')
dim(datacol)
head(datacol)
```

```{r}
require(skimr)
skim_without_charts(datacol)
```



The average level observed in the sample for cholesterol is 220.5 and presents a high variability
average around the arithmetic mean is 44.
The following section reports the descriptive analysis with respect to gender

```{r}
table(datacol$sex)
require(dplyr)
datacol%>%
dplyr::group_by(sex) %>%
skim_without_charts()

# or 
# tapply(datacol$cholst, datacol$sex, summary)
# tapply(datacol$cholst, datacol$sex, sd)
```

In the sample there are 97 women and 103 men and it is noted that the average cholesterol level for
for women it is 222.7 while for men it is lower at 218.4.
The average variability with respect to the arithmetic mean is equal to 42 for women and 45 for i
males.
The following representation shows the observed values through a scatter plot:
male values are represented by the larger circle

```{R}
n <-dim(datacol)[1]
with(datacol,
symbols(x=1:n,
y=cholst,
circles=sex,
inches=1/30 ,
xlab = "id",
ylab = "Colesterolo",
bg="red",
fg=NULL))
```

### Estimation of the parameters of the mixture model
The library called mclust (Gaussian Mixture Modeling for Model-Based Clustering)
allows you to estimate the parameters of the mixture model with Gaussian components.

```{r}
require('mclust')
```

#### Mclust function
The main function of the mclust::Mclust library requires as arguments:
- the data frame of the data,
- i.e. the number of components of the mixture model (G),
- the modelNames = "E" option to specify whether the model components are assumed
with same variance, homoscedastic model or modelNames = "V" if assumed with
specific variance for each component, heteroscedastic model


```{r}
mod1 <- Mclust(datacol$cholst,
G = 2,
modelNames = "E")
```

Note that the tolerance for the stopping criterion of the algorithm can be set with the option
control = emControl(tol = 1.e-6)

```{r}
summary(mod1)
```


The summary function returns:
- the value of the log-likelihood function at convergence after the parameters
were estimated with the EM algorithm.
- number of model parameters free to vary (in the model with two components
are given by the two means, the variance and the weight).


Parameter estimates are requested with
```{R}
summary(mod1,parameters = TRUE)
```


Where:
- Mixing probable are the estimates of the weights $\hat{\pi}$ of each component, $\hat{\pi_1}= 0.78 $ for the first and $\hat{\pi_2}= 0.22$ for the second.

- means the values of the averages of the two components. From the estimates it can be seen that the first
component is represented by a Normal random variable with mean value $\hat{\mu_1}= 204$, while the second
$\hat{\mu_2} = 279$

- Variances: the estimates of the variances of the components. Since by hypothesis they are the same
this is the estimate of a single variance $\hat{\sigma^2} = 936$. This indicates that on average
both in the first and second subpopulation the average variability with respect to the value
average is 30 mg per dl of plasma. 


From the estimation of this mixture model we know that the population is characterized by
a mixture of two subpopulations the first described by $X_1 \sim N(204, 936)$ and the second is $X_2 \sim N(279, 936)$

Classification of units
- clustering table returns the total number of patients that have been classified
as belonging to the first or second component.

In the observed sample of 158 patients, 42 patients are classified in the first group
the remainder in the second.

Note that this assignment is made with respect to the posterior probabilities that
are found in object z


```{r}
head(mod1$z)
```
According to this mixture model with two components the first unit has a probability
almost certain on the first component. The second unit, on the other hand, has a posterior probability
greater than being classified in the second component. While the fourth unit has a
probability of 0.75 of being classified in the first component and 0.25 of being classified
in the second component.
Based on this uncertainty of the a posteriori classification, the term of is calculated
penalty present in the ICL information criterion, reported in the model summary.
Note that it is possible to adopt the criterion of the maximum a posteriori probability that will come
shown below also applying the which.max function


```{r}
head(apply(mod1$z,1,which.max))
```

which returns the vector of elements 1 or 2 if the id is classified in the first or in the
second component respectively.
The following graph instead allows you to view all the observed values of black cholesterol,
and which values are classified into group 2 red and group 1 blue.

```{r}
plot(mod1,
what='classification', xlab = "colesterolo")
```

From the graph you can see the cholesterol value that discriminates the two subpopulations is slightly
above 250.
We can see how the units were classified according to male or female gender

```{r}
class<-mod1$classification; head(class)
table(class,datacol$sex)

```


The two genders are present in almost equal numbers in the two groups.
78% of the subjects belong to the first component.
#### Representation of mixture density
The mixed density graph is obtained as follows

```{r}
plot(mod1,
what='density', xlab = "Colesterolo")
```

### Choice of the number of components
The mclus:mclustBIC function allows you to display the information criterion values
Bayesian (BIC) when mixture models with numeric components are estimated
growing.
The BIC is calculated with respect to mixture models that differ in the number of components
depending on whether these have the same variance (option E) or different variability (option V).
The function takes the data vector as input


```{R}
bayesinf <- mclustBIC(datacol$cholst)
bayesinf
```

The table lists the realized BIC values for each mixture model with a
number of components varying from 1 to 9 (rows). The first line refers to the model without
heterogeneity or without components that assumes a homogeneous population.
The first column refers to models in which equal variability is assumed for the components
of the mixture while the second to models in which specific variability is assumed for each
component.
The best model is the one that has the minimum BIC value or with the highest
log-likelihood relative to the number of parameters and the number of observations.
The three best models with respect to this criterion are displayed below with the writing
top 3 models.
The most parsimonious model that explains the variability present in the data is the one with 2 components
and same variance, which is precisely the model estimated previously.
The table values can also be represented graphically as follows.


```{r}
plot(bayesinf)
```



### Multivariate mixture model with Gaussian components
The values representing the findings compared to two clinical tests performed are considered
on 200 patients (simulated data). In the data.Rdata file, assume that $Y_1$ represents the level
of white blood cells and $Y_2$ the hemoglobin level.

```{r}
load("data/data.Rdata")
head(data)
```

For the first patient the reading of white blood cells is 185 while that of hemoglobin
of 223.

```{r}
require(skimr)
skim_without_charts(data)
```

Patients have an average white blood cell level of 206 and hemoglobin level of 212. Variability
average around the average is around 37 for white blood cells and 36 for hemoglobin.

```{r}
cov(data)
cor(data)
```


The sample linear correlation coefficient is 0.6 and suggests a positive association
quite high between the two variables.
The two graphs highlight this association

```{r}
plot(data$Y1.1, data$Y2.1, xlab = "Globuli Bianchi",
ylab = "Emoglobina", col = "orange")
```



```{r}
plot(data$Y2.1, data$Y1.1,
xlab = "Emoglobina", ylab = "Globuli Bianchi", col = "blue")
```

The multivariate mixture model is estimated with the mclust library using decomposition
spectral for the variance-covariance matrix of the components. Initially
the spherical model is considered.
The following specifications EII and VII define the spherical model or the model in which
the components are independent with the same variance E or different variance V.

## Selection of the number of components
As in the univariate case, the selection of the components occurs with the information criterion
Bayesian in the following way


```{r}
require(mclust)
mcc <-Mclust(data, modelNames = c("EII", "VII"))
mcc$BIC
```
Evaluating a few different models for an increasing number of components with equal variance
or different the criterion suggests 3 components with equal variance (E).
## Parameter estimation
The model selected as best is estimated as follows

```{r}
mc <-Mclust(data,
G = 3,
modelNames = c("EII"))
summary(mc,parameters = TRUE )
```


The parameters free to vary in the model are 9 because the model is multivariate. The parameters
are the following:
- The weights of two components (mixing probabilities, the third is determined by difference
with respect to the sum constraint 1)
- The values of the means for the two variables in each subpopulation (6 means)
- The variance assumed to be equal for all components (1 variance)
From the estimated values we note that:
- The weight of each group is as follows 49% for the first group, 28% for the second, and 23%
for the third.
- The third subpopulation (group 3) is made up of those who have the highest average values
low for both variables.
- The first subpopulation represents those who have average white blood cell values
and hemoglobin 202 and 212 respectively.
- The second subpopulation is characterized by those who have the highest average values
and equal to 245 and 247.
- The average variability around the mean is the same for all components and is 23.

The classification chart below shows how drives are allocated into the three groups.


```{r}
plot(mc,"classification", xlab = "globuli bianchi",
ylab = "emoglobina")
```

The ellipses represent the univariate density of each component: they have the same shape with
focus corresponding to the estimated group means and have no orientation.
The mixture density contour lines are obtained as follows.

```{r}
plot(mc,"density", xlab = "globuli bianchi", ylab = "emoglobina")
```

The greatest density is concentrated at the point of intersection of the sample means. The
ellipses are oriented to the right since there is a positive correlation between the two variables.
### Mixture model with spherical components and specific variances for each component
The model with non-homogeneous distribution between groups postulates different variances for each
component and is specified with option VII

```{r}
mc1 <-Mclust(data, G = 3, modelNames = c( "VII"))
summary(mc1, parameters = TRUE)
```


From the results we note that:
- The parameters free to vary are 11 or 3 more than the model estimated in
precedence.
- The BIC value is higher than that of the previous model
- The percentage of patients belonging to the first component is lower than that
estimated in the previous model
- The weights of the components are not very different from those of the previous model
- The average values of each component are similar to those estimated previously
- The second component represents the subpopulation of patients with higher values
white blood cells and hemoglobin but also with greater variability than the other two
components. The estimated standard deviation is 26.
- The width of the ellipse of group 3 (red) is larger than that of group 2
(blue), due to the different variability and due to the hypothesis of a spherical model the ellipses do not have
orientation.

Note that the following graph shows ellipses of different widths representing the three
components having different variability.

```{r}
plot(mc1,"classification")
```

## Non-spherical mixture model
In the event that a covariance between the response variables other than zero is also assumed
specific for each component, the mixture model is estimated with the VEE option which assumes the
same shape for the densities and same orientation

```{r}
mc2 <-Mclust(data, G = 3, modelNames = c( "VEE"))
summary(mc2, parameters = TRUE)
```


From the results it can be seen that

- the parameters free to vary are 13 or 2 more than the previous model (5 in
more than in the initial model), the covariance between the response variables is greater in the
second component (the one in which the means of the two variables are greater)

```{r}
plot(mc2,"classification")
```


## Bootstrap for standard errors and confidence intervals
The confidence intervals for the model parameters are obtained by applying bootstrap
the percentile method.
Below we illustrate the application of the bootstrap for the first model that takes on structure
spherical and absence of correlation (EII).
The mclust::bootClust function allows you to get the boostrap sampling results
using by default ðµ = 999 bootstrap samples

```{r}
bootClust <- MclustBootstrap(mc)
```

The summary function returns the results by reporting the estimated standard errors for each
parameter

```{r}
summary(bootClust, what = "se")
```

and the confidence intervals calculated at the 95% confidence level.

```{r}
summary(bootClust, what = "ci")
```

## Latent class model
Consider the following data regarding the responses to two tests administered to patients of
an oncology department to evaluate depression and anxiety. (The data is taken from Zigmond,
A. and Snaith, R. (1983), The hospital anxiety and depression scale, Acta Psychiatrika
Scandinavica, 67, 361-370.)

```{r}
load("data/psico.Rdata")
dim(Y)

head(Y)

tail(Y)
```


The observations in the first column refer to the first item of the questionnaire it measures
depression and those in the second column refer to the second item which measures anxiety.
The items are measured on an ordinal scale with four levels coded from 0 to 3: 0 low,
1 medium-low, 2 medium, 3 high.
201 patients responded to the questionnaire and the relative frequency of each category can be
displayed with the apply function as follows


```{r}
n<-dim(Y)[1]
apply(Y,2,table)/n
```

Dalle frequenze relative per ogni categoria di risposta si nota che il 52% dei pazienti ha una
depressione medio-bassa ed il 46% un livello di ansia medio-basso. Solo il 4% presenta alti
livelli di depressione e di ansia.

Il modello a classi latenti con distribuzione non parametrica per la variabile latente con
supporto discreto e supposta con un numero di punti di supporto finito viene stimato con la
libreria MultiLCIRT (Multidimensional Latent Class Item Response Theory) e la funzione
est_multi_poly.

La funzione richiede che si determini il pattern delle configurazioni di risposta utilizzando la
funzione aggr_data nel modo seguente

```{R}
require(MultiLCIRT)
Yout <- aggr_data(Y)
S <- Yout$data_dis;S

yv <- Yout$freq; yv

cbind(S,yv)
```

The S object returned by the function contains the joint configurations of the levels of the two
response variables, while the vector yv associates the respective sample frequencies. Therefore
there are 79 patients who have a medium-low level of depression and anxiety. There are 4
patients who present high levels of depression and anxiety.
The function to estimate the model requires S and yv and requires you to specify the number of classes
latent ð‘˜. Assuming for the moment the existence of two latent classes, the function returns
the following output

```{r}
mod2 <- est_multi_poly(S,yv,k=2)

summary(mod2)

mod2$np
```


The value of the convergence log-likelihood of the EM algorithm is -374.62 and the model
with two latent classes it has 13 parameters free to vary, and the value of the information criterion
Bayesian is 818.18.
From the parameter estimates we note that the weight of the first component is 0.48 while that
of the second component is 0.52.
From the estimates of the conditional probabilities we can characterize the first latent class
such as that referring to the subpopulation of patients at low risk of depression and anxiety
as the probability of reporting a high level of depression or anxiety is zero and is
the probability of responding in the first category of both variables is high.
The second latent class represents more than half of the patient population and is
characterized by those who are more prone to depression and anxiety because
the probability of the first category is zero while the probability for the remaining ones is higher
categories.
The classification of patients into the two groups occurs with a posteriori probabilities. This
is required by specifying output = TRUE in the estimate function and then the probabilities are
contained in the Pp object of the model.

```{r}
mod2 <- est_multi_poly(S,yv,k=2,
output = TRUE)
round(mod2$Pp,2)
```

We note that for those who have a response pattern of 1,1 (first row of the S matrix) the
estimated a posteriori probabilities are 0.036 and 0.964 of belonging to the first and second
latent class respectively. With the rule of maximum a posteriori probability these
patients are allocated to the second latent class.
Considering those who have response patterns 1, 2 (line 7 of S) we notice that in line 7
of Pp we have 0.126 and 0.874. So those who have a medium-low level of depression and
a medium level of anxiety are allocated with probability 0.9 in the second latent class.





# Exercise:

## Realizations from the Bivariate Normal distribution
Exercise 31
1. Obtain first 10 and then 3000 realizations from a Normal distribution double $(ð‘‹_1, ð‘‹_2) \sim ð‘ (\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \sigma_{12}$ with the following parameter values:
$ð‘ (0, 0, 3, 3, 0)$. Set the seed to the value 14263.
2. Describe the data generated considering ð‘› = 3000 and draw the
empirical distribution. Yes comments.
3. Draw the scatter plot of the realizations and comment on them.
4. Draw and interpret the level curves obtained from the theoretical distribution.

31.1.
To obtain pseudo-random realizations from a Bivariate Normal Random Variable you use the rmvnorm function from the mvtnorm package. This command
requires as arguments the number of values to generate (n), the vector of means (mean) and the
variance covariance matrix (sigma).

```{r}
require(mvtnorm)
medie <- c(0, 0)
varcov <- matrix(c(3, 0, 0, 3), ncol = 2, byrow = TRUE)
set.seed(14263)
x <- mvtnorm::rmvnorm(n = 3000, mean = medie, sigma = varcov)
```

31.2.
The object returned in output from the rmvnorm function is a matrix of two columns and 3000 rows: the
two columns represent the two components $(ð‘‹_1 and X_2)$ of the bivariate variable, while each
row corresponds to a different observation.
We calculate the sample values of the means and the variance covariance matrix with the corresponding ones
theorists.

```{r}
apply(x, 2, mean)

cov(x)

```

We observe, for all the indices considered, a good adherence of the empirical values obtained to those
of the theoretical distribution.

We also note that, in this case, the covariance between the two components is equal to 0, i.e
two components are uncorrelated. It follows that they are also independent: the first component
(first column) has Univariate Normal distribution with mean 0 and variance 3, i.e. ð‘‹1 âˆ¼ ð‘ (0, 3),
while the second component (second column) in turn has Univariate Normal distribution
with mean 0 and variance 3, i.e. ð‘‹2 âˆ¼ ð‘ (0, 3), independent of the first. It is important to underline
that this result does not hold in general if the two components are correlated, that is, if ðœŽ12 â‰  0.
In conclusion, we represent the two empirical distribution functions in the same graphic window.


```{r}
par(mfrow=c(1, 2))
plot(ecdf(x[, 1]),
do.points = FALSE,
xlim = c(-8, 8),
main = 'Fun. Rip. Emp. prima componente')
plot(ecdf(x[, 2]),
do.points = FALSE,
xlim = c(-8, 8),
main = 'Funz. Rip. Emp. seconda componente')
```

Confirming the fact that the two components can be considered as generated by the same one
univariate Gaussian distribution (ð‘ (0, 3)), the two curves are substantially indistinguishable from each other
on the other; it can also be observed that they follow the function curve practically perfectly
of theoretical distribution for a V.C. corresponding.

31.3.

We visualize the scatter graph by simply plotting the vectors relating to the two in the plane
components of the bivariate distribution (x-axis: first component, y-axis: second component).

```{r}
plot(x[, 1], x[, 2],
col = "orange",
asp = 1,
main = "Diagramma a dispersione",
xlab = "Prima componente",
ylab = "Seconda componente")
```

The asp = 1 option allows you to use the same unit of measurement for the x axis and y axis. This
allows you to precisely visualize the shape of the point cloud, highlighting in particular
the differences between the two components and the relationships between them. In this case yes
observe that the point cloud takes on a perfectly circular shape, a consequence of the fact that
the variances $\sigma_1^2$ and $\sigma_2^2$
 are equal and the covariance $\sigma_{12}$ is equal to $0$. The range of variation for the two
components is approximately the same, varying in both cases from a minimum of approximately
- 6 to a maximum of approximately 6. Finally we observe that the points are more distributed in the
central area, corresponding to the point given by the averages of the two components, with a density that
it gradually decreases as you move away from the center.

31.4.

The graphical representation of the density of a V.C. Bivariate normal is obtained through a
three-dimensional representation (three-dimensional space). A 2D representation can be
obtained by intersecting the three-dimensional surface with planes placed at different heights and parallel to each other
They. The result is the so-called level curves. Practically, in R, the steps to follow are i
following:
- a (2D) grid of points is defined: in this case we select the vectors z1 and z2 as one
sequence of equally spaced points from -10 to 10. The expand.grid command allows you to obtain
the grid by constructing all possible pairs of points $(z_1[i], z_12[j])$;
- the density value is calculated at each of the grid points: the dmvnorm function (in mode
analogous to the dnorm function in the univariate case) requires as input the grid of points, the vector
of the means and the variance-covariance matrix and returns the density at the grid points
in vector form. It is subsequently necessary to transform it into a matrix object;
111
- finally, the contour command is used to plot the density values just calculated.


```{r}
z1 <- z2 <- seq(-10, 10, by = 0.1)
griglia <- expand.grid(z1, z2)
dens <- matrix(dmvnorm(griglia,
mean = medie,
sigma = varcov),
ncol = length(z1))
contour(z1, z2, dens,
asp = 1,
# nlevels = 3,
# levels = c(0.01, 0.02, 0.04, 0.07),
main = "Curve di Livello", xlab = expression(X[1]), ylab = expression(X[2]),
col = "dodgerblue")
```


By default the contour function automatically establishes the level curves to represent; And
However, it is possible to specify the number of curves you want to obtain or even the specific values of the
odds. As already observed in the scatterplot, we notice that the level curves have shape
perfectly circular (equal axis length and consequently no specific orientation).
The center of each circle is located at the point (0,0), corresponding to
of the averages of the two components.


## Exercise 32
Consider the following distribution $(ð‘‹_1$, X_2) âˆ¼ ð‘ (\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \sigma_{12}) Bivariate Normal
$ð‘ (2, 2, 100, 100, 4)$ and repeat the points from the previous exercise. In the representation
of the contour lines, pay attention to the definition of the grid for
the density calculation.

```{r}
require(mvtnorm)
medie <- c(2, 2)
varcov <- matrix(c(100, 4, 4, 100), ncol = 2)
set.seed(14263)
x <- rmvnorm(n = 3000, mean = medie, sigma = varcov)
```

2.

Descriptive statistical analyzes (in this case the covariance is different from 0, therefore the two components
cannot be considered independent).


```{r}
mean(x)

cov(x)
```


Graph of empirical distribution functions

```{r}
par(mfrow = c(1, 2))
plot(ecdf(x[, 1]),
do.points = FALSE,
main = 'Funz. Rip. Emp. prima componente')
plot(ecdf(x[, 2]),
do.points = FALSE,
main = 'Funz. Rip. Emp. seconda componente')
```

.3

Scatter plot of the two components.

```{R}
plot(x[, 1], x[, 2],
col = "green",
asp = 1,
main = "Diagramma a dispersione",
xlab = "Prima componente",
ylab = "Seconda componente")
```


Although it is impossible to detect it precisely from the graph, it is known that the point cloud is in
in this case centered in (2, 2), corresponding to the averages of the two components. It is also difficult to detect
the orientation of the point cloud, which we know is upwards (the major axis of the ellipse
is arranged along a straight line with a positive angular coefficient). The cause is the value of the variances that
is significantly higher than that of the covariance. In fact, it is observed that the range of variation of
two components is much higher than what was noted in the previous exercise. Let's reiterate
finally that, although the shape of the point cloud appears to be a circumference, it is this
case an ellipse, given the value of the covariance different from 0.


##.4 

The procedure is identical to the one introduced in Exercise 31; we only observe that the vectors z1
and z2 must be defined differently: first, in the previous exercise the averages were
equal to 0, therefore the two vectors were symmetrical with respect to 0 (they went from -5 to 5); now the averages are
equal to 2, therefore z1 and z2 must be symmetrical with respect to 2. Secondly, the variability
it was very limited in the previous year (variances equal to 3); in this case we have one instead
much greater variability (variances equal to 100): it is therefore necessary that the intervals represented
from z1 and z2 are much wider.


```{R}
z1 <- z2 <- seq(-20, 24, by = 0.1)
griglia <- expand.grid(z1, z2)
dens <- matrix(dmvnorm(griglia,
mean = medie,
sigma = varcov),
ncol = length(z1))
contour(z1, z2, dens,
   asp = 1,
# nlevels = 3,
# levels = c(0.01, 0.02, 0.04, 0.07),
main = "Curve di Livello", xlab = expression(X[1]), ylab = expression(X[2]),
col = "dodgerblue")    
```


The interpretation and comments relating to the graph mostly follow those of the previous year;
we simply observe that the values of the height of the parallel planes are significantly lower
compared to those shown in the previous year. This testifies to a three-dimensional shape
of the very lowest distribution, flattened and extended in the plane.



# Soluzioni esercizi â€œApplicazione dellâ€™algoritmo EMâ€
## SOLUZIONI ESERCIZIO 33

Consider the example shown in the handouts of the usage part of the applications
of the EM algorithm to impute the missing values of the contingency table. Yes
consider the following observations for the absolute frequencies: $ð‘¦_{11} = 16, ð‘¦_{12} = 7,
ð‘¦_{13} = 7, ð‘¦_{21} = ð‘ ð´, ð‘¦_{22} = 64, ð‘¦_{23} = 5.$

1. Describe the functions that allow you to estimate the model parameters
generalized linear for frequencies using the EM algorithm.

2. Describe the iterative process through the parameter estimates graph
obtained at each iteration. Report and comment on the parameter estimates
and the missing value.

3. Change the input parameter of the function to stop the algorithm
having less than 20 iterations. Justify the result compared to the new ones
parameter estimates.

.1

Si considera una tabella di contingenza costituita da 2 righe e 3 colonne e contenente un valore
mancante (NA). Si utilizza lâ€™algoritmo di Expectation-Maximization (EM) per stimare il dato mancante
sulla base di un modello lineare $ð‘¦_{ð‘–ð‘—} = \mu + \alpha_i + \beta_i + \epsilon_{ij}$, dove $\mu$ Ã¨ lâ€™intercetta, $\alpha_i$ Ã¨ il parametro
di riga, $\beta_j$ Ã¨ il parametro di colonna e $\epsilon_ij \sim ð‘ (0, \sigma_2)$ Ã¨ un termine di errore.

Lâ€™algoritmo EM Ã¨ costituito da due passi (si vedano le dispense di applicazioni per maggiori dettagli):


- step E (expectation): by assigning an initial value to the missing value, the values are calculated
of the model parameters;
- M step (maximization): the model parameters are used to estimate and update the
value of the missing data. The em1 function defined below implements the two steps described;
returns as output the value of all the estimated quantities.

```{r}
em1 <- function(y21, y){
ystar <- y
ystar[2, 1] <- y21
# Expectation Step
mu.hat <- mean(ystar)
alpha.hat <- apply(ystar, MAR = 1, mean) - mean(ystar)
beta.hat <- apply(ystar, MAR = 2, mean) - mean(ystar)
# Maximization Step
y21 <- mu.hat + alpha.hat[2] + beta.hat[1]
return(c(mu = mu.hat,
alpha = alpha.hat,
beta = beta.hat,
y21 = y21))
}
y <- matrix(c(16, 7, 7, NA, 64, 5), nrow = 2, ncol = 3, byrow = TRUE); y
```


```{r}
em1(19.8, y)
```

By executing the function, setting the value of 19.8 (even) as the initial value for the missing data
to the average of the present data), the values shown above are obtained. In particular the value of
missing frequency is estimated to be 27.7, thus updating the initially assigned value of
19.8.


The EM algorithm involves iterating the procedure by iterating these two steps until convergence. Yes
it then iterates the execution of the em1 function through a while loop until the estimate of
parameters remains â€œstableâ€, i.e. until the difference between the value of the two parameters
subsequent iterations of the algorithm is less than a certain threshold ðœ€ (10âˆ’8 in the example below).
```{r}
em.step <- function(y, epsilon = 1e-8) {
trace <- NULL
convergenza <- FALSE
trace <- t(em1(y21 = mean(y, na.rm = TRUE), y = y))
y21id <- grep("y21", colnames(trace))
i <- 0
while(!convergenza) {
i <- i + 1
trace <- rbind(trace, em1(y21 = trace[i, "y21"], y = y))
convergenza <- (dist(trace[i:(i+1), -y21id]) < epsilon)
}
return(trace)
}
set.seed(183)
est <- em.step(y)
```

The generated output is a matrix of 51 rows and 7 columns. Each column represents a different
parameter estimated by the model. The last column in particular is the value obtained for the data
missing. Each row instead contains the estimates obtained at one iteration of the algorithm. For
reaching convergence the EM algorithm therefore requires 51 iterations. Particularly in the last line
we find the values of the convergent parameters (therefore "definitive"). The value of the missing data
estimated through the EM algorithm is therefore equal to 43.5 (it must be approximated, since it is
a frequency).

.2

Since reading the output matrix is not easy, we can represent the values
obtained through a graph (trace plot). The matplot function is used which allows you to represent
each of the data series contained in the matrix (we exclude the estimated value for the
missing frequency).

```{R}
names1 <- expression(mu, alpha[1], alpha[2], beta[1], beta[2], beta[3])
pal1 <- c("red", "yellow", "green", "violet", "blue", "orange")
matplot(est[, -7],
type = "l",
col = pal1,
lwd = 2,
lty = 1,
xlab = "Iterazioni dell'algoritmo EM",
ylab = "Stime dei parametri del modello")
legend(x = 0,
y = -5,
legend = names1,
lwd = 2 ,
col = pal1,
lty = 1,
horiz = TRUE,
cex = 0.5)
```


We observe that the parameters ðœ‡, ð›¼2 and ð›½3 have an increasing trend as the iterations increase
of the algorithm, unlike the remaining parameters. It is also noted that the EM algorithm, by updating
the values of the estimates from one iteration and the following, involves a sensitive adjustment during
the first steps (before the tenth), while the adjustments are much more limited for the steps
subsequent ones.

.3

Below we modify the em.step function in such a way that the stopping criterion of the EM algorithm does not
both the convergence of the estimated parameters, but the achievement of a maximum number of iterations
(NB. In practice the two conditions are often combined: convergence is required
achieved, but if after a maximum number of iterations, the EM algorithm is still "running",
then it is blocked anyway. This maximum number of iterations is usually quite high.
It is absolutely not recommended to use only having achieved the algorithm as a stop condition
a maximum number of iterations.). No changes to the em1 function are needed. In function
em.step only changes the definition of the convergence object (renamed maxit_reached).



```{r}
em.step <- function(y, maxit = 20) {
trace <- NULL
maxit_reached <- FALSE
trace <- t(em1(y21 = mean(y, na.rm = TRUE), y = y))
y21id <- grep("y21", colnames(trace))
i <- 0
while(!maxit_reached) {
i <- i + 1
trace <- rbind(trace, em1(y21 = trace[i, "y21"], y = y))
maxit_reached <- (i >= (maxit-1))
}
return(trace)
}
est_mod <- em.step(y, maxit = 20)
```


therefore there is no guarantee that the parameter estimates are actually reliable (in this
case no significant differences are found, but the estimates obtained with the second method are
however less precise).



## Solutions to the â€œMixture densityâ€ exercises
## DensitÃ  miscuglio
## Esercizio 36
1. Si utilizzi la funzione â€˜funcmxnâ€˜ implementata a lezione per rappresentare graficamente
i valori della densitÃ  della funzione miscuglio considerando il seguente
scenario: $ð‘‹_1 \sim ð‘ (1, 1.3), ð‘‹_2 \simð‘ (0, 1.3)$ e peso per la prima componente
$ð‘ = 0.32$.

2. Si crei la funzione chiamata â€˜rnormxnâ€˜ che genera 1000 realizzazioni da entrambe
le componeti definite al punto precedente e restituisce le corrispondenti
realizzazioni del miscuglio di cui sopra.

3. Si riporti lâ€™istogramma delle realizzazioni ottenute al punto .2 e si commenti il
risultato.







## SOLUTIONS TO EXERCISE 36

.1

Let's consider the funcmxn function implemented in class to graph the function
of density of a mixture with two Gaussian components.


```{R}
funcmxn <- function(x, p, mu, sd){
f1 <- dnorm(x, mu[1], sd[1])
f2 <- dnorm(x, mu[2], sd[2])
f <- p*f1 + (1-p)*f2
f
}
```

We define the vectors for the means and standard deviations and the weight value of the first component;
the function funcmxn determines the value of the density at the point (x) that is passed to it as input.

```{r}
mu <- c(1, 0)
sd <- c(sqrt(1.3), sqrt(1.3))
p <- 0.32
funcmxn(0.5, p, mu, sd)
```

Per esempio, nel punto ð‘¥ = 0.5 la funzione di densitÃ  del miscuglio Gaussiano considerato ha valore
pari a 0.32. Una rappresentazione grafica dei valori della desnitÃ  consenste di valutare meglio il
miscuglio considerato. Per ottenere il grafico Ã¨ necessario determinare la densitÃ  in un intervallo
di punti. Consideriamo quindi una successione di numeri equispaziati da -5 a 10 (in generale
gli estremi devono essere valutati caso per caso ed eventualmente aggiustati dopo una visione
preliminare del grafico). La successione di punti viene poi passata in argomento alla funzione
funcmxn. Aggiungiamo le righe tratteggiate in corrispondenza delle medie delle due componenti.

```{r}
y <- seq(-5, 6, by = 0.01)
pr <- funcmxn(x = y, p = p, mu = mu, sd = sd)
plot(y, pr,
xlab = "y", ylab = "DensitÃ ",
lwd = 3,
col = "lightsteelblue1",
type = "l",
main = "Miscuglio di peso 0.32 tra N(1, 1.3) e N(0, 1.3)")
abline(v = c(0, 1), lty = c(2, 2), col = c("red", "red"))

```

In this case the density graph does not allow the two components to be distinguished; is observed
in fact a single maximum, with a central peak apparently symmetrical with respect to a point
between 0 and 1 (the averages of the two components). The average values of the two components
in fact they are too close to each other (in relation to a variance greater than 1), to be consistent
to distinguish the two peaks. It would be necessary to increase the distance between the two averages or decrease it
the value of the variance.

2.

We define the function rnormxn which generates pseudo realizations from a mixture function with
two Gaussian components. Similarly to the previous function, it requires the values of the as input
weight of the first component (w) and the vectors of the means and variances of the two components. Everything is fine
observation is extracted with probability equal to w from the first component and with equal probability
1-w from the second component.

```{r}
rnormxn <- function(n, w, mu, sd){
x = numeric(n)
for(i in 1:n){
dm = runif(1, 0, 1)
if(dm < w)
x[i] = rnorm(1, mu[1], sd[1])
else
x[i] = rnorm(1, mu[2], sd[2])
}
x
}

val <- rnormxn(1000, w = p, mu = mu, sd = sd)
summary(val)

var(val)

```

We calculate the descriptive statistics of the 1000 pseudo-realizations obtained from the mixture function.
The range of variation of the observed data goes from a minimum of -3.3 to a maximum of 4.3.
Mean and median are approximately coincident: half of the observations take on a value
less than 0.35. In general, the symmetry already observed in the graph seems to be confirmed
of the density of the mixture. The variance value is 1.44, slightly higher than the (common) value
of the two Gaussian components of the mixture.

.3

The histogram of the realizations obtained in the previous point substantially confirms the observation
of the previous point: the range of variation is rather limited, the (single) center of the
distribution is slightly above 0, and the shape of the histogram is approximately symmetrical.
The presence of a double maximum corresponding to the two components of the mixture is not noted.


```{r}
hist(val,
breaks = 25,
ylab = "Frequenze assolute",
xlab = "Realizzazioni",
main = "Pseudo-det. miscuglio di peso 0.32 e N(1,1.3), N(0,1.3)")
```


Ecsercise 37 ???
















